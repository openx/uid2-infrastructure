version: 2.1

# Branch filter
# https://discuss.circleci.com/t/can-you-filter-on-a-workflow-level/30624/5
dev_only: &dev_only
  filters:
    branches:
      only:
        - live-dev
qa_only: &qa_only
  filters:
    branches:
      only:
        - live-qa
prod_only: &prod_only
  filters:
    branches:
      only:
        - live-prod
live_only: &live_only
  filters:
    branches:
      only:
        - live-prod
        - live-qa
        - live-dev

workflows:
  version: 2
  plan_approve_apply:
    jobs:
      - plan-stage1
      - hold-stage1:
          type: approval
          requires:
            - plan-stage1
          <<: *prod_only
      - apply-stage1:
          requires:
            - plan-stage1
            - hold-stage1
          <<: *live_only
      - plan-stage2:
          requires:
            - apply-stage1
          <<: *live_only
      - hold-stage2:
          type: approval
          requires:
            - plan-stage2
          <<: *prod_only
      - apply-stage2:
          requires:
            - plan-stage2
            - hold-stage2
          <<: *live_only
      - plan-destroy:
          requires:
            - apply-stage2
      - hold-destroy:
          type: approval
          requires:
            - plan-destroy
      - destroy:
          requires:
            - hold-destroy

jobs:
  plan-stage1:
    working_directory: /tmp/project
    docker:
      - image: docker.mirror.hashicorp.services/hashicorp/terraform:0.14.11
    steps:
      - checkout
      - run:
          name: set live environment name based on branch name
          command: |
            echo -n 'export environment=' >> $BASH_ENV
            echo $CIRCLE_BRANCH | sed 's/live-//' | awk '{ if ($1 == "dev" || $1 == "qa" || $1 == "prod") print $1; else print "dev"; }' >> $BASH_ENV
      - run:
          name: terraform init & plan stage1
          command: |
            . $BASH_ENV
            eval export GOOGLE_PROJECT=\$${environment}_GOOGLE_PROJECT
            eval export GOOGLE_CREDENTIALS=\$${environment}_GOOGLE_CREDENTIALS
            eval export AWS_ACCESS_KEY_ID=\$${environment}_AWS_ACCESS_KEY_ID
            eval export AWS_SECRET_ACCESS_KEY=\$${environment}_AWS_SECRET_ACCESS_KEY
            export TF_VAR_environment=${environment}
            export TF_VAR_regions=`eval echo \\$${environment}_TF_VAR_regions_B64 |base64 -d`
            eval echo \$${environment}_TERRAFORM_BACKEND_B64 | base64 -d > terraform/stage1/backend.tf
            export AWS_DEFAULT_REGION=us-west-1
            terraform -chdir=terraform/stage1 init -input=false
            terraform -chdir=terraform/stage1 plan -out tfapply
      - persist_to_workspace:
          root: .
          paths:
            - .
  apply-stage1:
    docker:
      - image: docker.mirror.hashicorp.services/hashicorp/terraform:0.14.11
    steps:
      - attach_workspace:
          at: .
      - run:
          name: set live environment name based on branch name
          command: |
            echo export environment=$CIRCLE_BRANCH | sed 's/live-//' >> $BASH_ENV
      - run:
          name: terraform apply stage1
          command: |
            . $BASH_ENV
            eval export GOOGLE_PROJECT=\$${environment}_GOOGLE_PROJECT
            eval export GOOGLE_CREDENTIALS=\$${environment}_GOOGLE_CREDENTIALS
            eval export AWS_ACCESS_KEY_ID=\$${environment}_AWS_ACCESS_KEY_ID
            eval export AWS_SECRET_ACCESS_KEY=\$${environment}_AWS_SECRET_ACCESS_KEY
            export AWS_DEFAULT_REGION=us-west-1
            eval echo \$${environment}_TERRAFORM_BACKEND_B64 | base64 -d > terraform/stage1/backend.tf
            terraform -chdir=terraform/stage1 apply -auto-approve tfapply
      - persist_to_workspace:
          root: .
          paths:
            - .
  plan-stage2:
    working_directory: /tmp/project
    docker:
      - image: docker.mirror.hashicorp.services/hashicorp/terraform:0.14.11
    steps:
      - attach_workspace:
          at: .
      - run:
          name: set live environment name based on branch name
          command: |
            echo export environment=$CIRCLE_BRANCH | sed 's/live-//' >> $BASH_ENV
      - run:
          name: terraform init & plan stage2
          command: |
            . $BASH_ENV
            eval export GOOGLE_PROJECT=\$${environment}_GOOGLE_PROJECT
            eval export GOOGLE_CREDENTIALS=\$${environment}_GOOGLE_CREDENTIALS
            eval export AWS_ACCESS_KEY_ID=\$${environment}_AWS_ACCESS_KEY_ID
            eval export AWS_SECRET_ACCESS_KEY=\$${environment}_AWS_SECRET_ACCESS_KEY
            export TF_VAR_environment=${environment}
            export TF_VAR_regions=`eval echo \\$${environment}_TF_VAR_regions_B64 |base64 -d`
            eval echo \$${environment}_TERRAFORM_BACKEND_B64 | base64 -d |sed '/prefix/ s/"$/\/stage2\"/' > terraform/stage2/backend.tf
            export AWS_DEFAULT_REGION=us-west-1
            terraform -chdir=terraform/stage2 init -input=false
            terraform -chdir=terraform/stage2 plan -out tfapply
      - persist_to_workspace:
          root: .
          paths:
            - .
  apply-stage2:
    docker:
      - image: docker.mirror.hashicorp.services/hashicorp/terraform:0.14.11
    steps:
      - attach_workspace:
          at: .
      - run:
          name: set live environment name based on branch name
          command: |
            echo export environment=$CIRCLE_BRANCH | sed 's/live-//' >> $BASH_ENV
      - run:
          name: terraform apply stage2
          command: |
            . $BASH_ENV
            eval export GOOGLE_PROJECT=\$${environment}_GOOGLE_PROJECT
            eval export GOOGLE_CREDENTIALS=\$${environment}_GOOGLE_CREDENTIALS
            eval export AWS_ACCESS_KEY_ID=\$${environment}_AWS_ACCESS_KEY_ID
            eval export AWS_SECRET_ACCESS_KEY=\$${environment}_AWS_SECRET_ACCESS_KEY
            export AWS_DEFAULT_REGION=us-west-1
            eval echo \$${environment}_TERRAFORM_BACKEND_B64 | base64 -d |sed '/prefix/ s/"$/\/stage2\"/' > terraform/stage2/backend.tf
            terraform -chdir=terraform/stage2 apply -auto-approve tfapply
      - persist_to_workspace:
          root: .
          paths:
            - .
  fleet-install:
    docker:
#      - image: alpine/helm:3.5.2
      - image: kiwigrid/gcloud-kubectl-helm:3.3.4-312.0.0-267
    steps:
      - attach_workspace:
          at: .
      # Install/update CRDs to fleet manager
      - run:
          name: helm-fleet-crd
          command: |
            helm upgrade -n fleet-system --install --create-namespace --wait fleet-crd https://github.com/rancher/fleet/releases/download/v0.3.3/fleet-crd-0.3.3.tgz --kubeconfig ./terraform/outputs/gcp-kubecontext-mission-control.yaml
      # Install fleet manager to mission control cluster
      - run:
          name: fleet-manager
          command: |
            helm upgrade -n fleet-system --install --create-namespace --wait --values ./terraform/outputs/fleet-agent-values.yaml fleet https://github.com/rancher/fleet/releases/download/v0.3.3/fleet-0.3.3.tgz --kubeconfig ./terraform/outputs/gcp-kubecontext-mission-control.yaml
      # Create uid2 namespace
      - run:
          name: uid2-namespace
          command: |
            echo '{ "apiVersion": "v1", "kind": "Namespace", "metadata": { "name": "uid2" }}' | kubectl apply --kubeconfig="./terraform/outputs/gcp-kubecontext-mission-control.yaml" -f -
      # Generate cluster registration token
      - run:
          name: create-registration-token
          command: |
            echo '{"kind": "ClusterRegistrationToken", "apiVersion": "fleet.cattle.io/v1alpha1","metadata": {"name": "uid2-fleet", "namespace": "uid2"},"spec": {"ttl": "2h"}}' | kubectl apply --kubeconfig="./terraform/outputs/gcp-kubecontext-mission-control.yaml" -f -
      # Save cluster registration token
      - run:
          name: save-reegistration-token
          command: |
            kubectl -n uid2 get secret uid2-fleet -o 'jsonpath={.data.values}' --kubeconfig="./terraform/outputs/gcp-kubecontext-mission-control.yaml" | base64 -d | grep token: >> ./terraform/outputs/fleet-agent-values.yaml 
      # Install/update fleet-agents downstream
      - run:
          name: helm-fleet-agent-install-gcp-clusters
          command: |
            find ./terraform -name gcp-kubecontext-*.yaml  ! -name gcp-kubecontext-mission-control.yaml -print -exec \
            helm upgrade -n fleet-system --install --create-namespace --wait --values ./terraform/outputs/fleet-agent-values.yaml \
            --set labels.env=`echo $CIRCLE_BRANCH | sed 's/live-//'` --set labels.fleet=uid2 --set labels.cloud=gcp \
            fleet-agent https://github.com/rancher/fleet/releases/download/v0.3.3/fleet-agent-0.3.3.tgz --kubeconfig {} \;
      - run:
          name: helm-fleet-agent-install-aws-clusters
          command: |
            find ./terraform -name aws-kubecontext-*.yaml  ! -name gcp-kubecontext-mission-control.yaml -print -exec \
            helm upgrade -n fleet-system --install --create-namespace --wait --values ./terraform/outputs/fleet-agent-values.yaml \
            --set labels.env=`echo $CIRCLE_BRANCH | sed 's/live-//'` --set labels.fleet=uid2 --set labels.cloud=aws \
            fleet-agent https://github.com/rancher/fleet/releases/download/v0.3.3/fleet-agent-0.3.3.tgz --kubeconfig {} \;
      - run:
          name: install-fleet-gitrepo-mission-control
          command: |
            echo '{ "apiVersion": "fleet.cattle.io/v1alpha1", "kind": "GitRepo", "metadata": { "name": "mission-control", "namespace": "fleet-local"},
            "spec": {"repo": ' \"https://github.com/$CIRCLE_PROJECT_USERNAME/$CIRCLE_PROJECT_REPONAME\", '"paths": ["fleet/gcp/mission-control"], "branch": ' \"$CIRCLE_BRANCH\"}} | kubectl apply --kubeconfig="./terraform/outputs/gcp-kubecontext-mission-control.yaml" -f -
      - run:
          name: install-fleet-gitrepo-uid2
          command: |
            echo '{ "apiVersion": "fleet.cattle.io/v1alpha1", "kind": "GitRepo", "metadata": { "name": "uid2-infrastructure", "namespace": "uid2"},
            "spec": {"repo": ' \"https://github.com/$CIRCLE_PROJECT_USERNAME/$CIRCLE_PROJECT_REPONAME\", '"paths": ["fleet/common", "fleet/gcp/uid2"], "branch": ' \"$CIRCLE_BRANCH\",  \
            '"targets": [{"name": "all","clusterSelector": {"matchLabels": {"fleet": "uid2", "cloud": "gcp"}}}]}}' | kubectl apply --kubeconfig="./terraform/outputs/gcp-kubecontext-mission-control.yaml" -f -
      - persist_to_workspace:
          root: .
          paths:
            - .
  plan-destroy:
    docker:
      - image: docker.mirror.hashicorp.services/hashicorp/terraform:0.14.11
    steps:
      - attach_workspace:
          at: .
      - run:
          name: terraform create destroy plan
          command: |
            terraform -chdir=terraform plan -destroy -out tfdestroy 
      - persist_to_workspace:
          root: .
          paths:
            - .
  destroy:
    docker:
      - image: docker.mirror.hashicorp.services/hashicorp/terraform:0.14.11
    steps:
      - attach_workspace:
          at: .
      - run:
          name: terraform destroy
          command: |
            terraform -chdir=terraform apply -auto-approve tfdestroy
